# Project Log: micro.llama2.c

##  Registro: v1.0 - Correcci贸n de Inferencia y Par谩metros
- **Fallo/Motivo**: El comando `sample.py` fallaba con `AssertionError` y luego con `UnpicklingError`.
- **Causa**: 
    1. El `configurator.py` requiere el formato `--key=val` para los argumentos.
    2. PyTorch 2.6 cambi贸 el valor por defecto de `weights_only` a `True` en `torch.load`, lo que rompe la carga de checkpoints antiguos de este proyecto.
- **Soluci贸n/Cambio**: 
    1. Se instruy贸 al usuario a usar el formato `--key=val`.
    2. Se modificar谩 `sample.py` para incluir `weights_only=False` en la llamada a `torch.load`.

##  Registro: v1.1 - Archivo Corrupto Detectado
- **Fallo**: `_pickle.UnpicklingError: invalid load key, 'E'.`
- **Causa**: El archivo `stories260K.bin` tiene un tama帽o de 15 bytes y contiene el texto "Entry not found". Probablemente una descarga fallida de HuggingFace. Adem谩s, es un archivo `.bin` (C) intentando ser le铆do como `.pt` (PyTorch).
- **Soluci贸n**: Notificar al usuario y proporcionar links correctos para modelos `.pt`.

##  Registro: v1.2 - Inferencia Exitosa (Python)
- **Cambio**: Se descarg贸 el modelo `stories15M.pt` (58MB).
- **Resultado**: El script `sample.py` gener贸 texto correctamente: *"Once upon a time, there was a little girl named Lily..."*.
- **Estado**: La versi贸n de Python est谩 operativa.

##  Registro: v1.3 - Resoluci贸n de stories260K.pt
- **Fallo**: `AssertionError: data\tok512.model` e `IndexError`.
- **Causa**: 
    1. El modelo `stories260K` tiene un vocabulario de solo 512 tokens.
    2. Intentar usar el tokenizer de Llama2 (32,000 tokens) causa un error de 铆ndice en la capa de embeddings.
- **Soluci贸n**: Se descarg贸 el archivo `tok512.model` en la carpeta `data/`.
- **Resultado**: Inferencia exitosa con el modelo mini para pruebas.

# Backup
*(Aqu铆 se guardar谩n ideas descartadas o versiones anteriores en el futuro)*
