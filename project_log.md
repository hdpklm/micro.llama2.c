# Project Log: micro.llama2.c

##  Registro: v1.0 - Correcci贸n de Inferencia y Par谩metros
- **Fallo/Motivo**: El comando `sample.py` fallaba con `AssertionError` y luego con `UnpicklingError`.
- **Causa**: 
    1. El `configurator.py` requiere el formato `--key=val` para los argumentos.
    2. PyTorch 2.6 cambi贸 el valor por defecto de `weights_only` a `True` en `torch.load`, lo que rompe la carga de checkpoints antiguos de este proyecto.
- **Soluci贸n/Cambio**: 
    1. Se instruy贸 al usuario a usar el formato `--key=val`.
    2. Se modificar谩 `sample.py` para incluir `weights_only=False` en la llamada a `torch.load`.

##  Registro: v1.1 - Archivo Corrupto Detectado
- **Fallo**: `_pickle.UnpicklingError: invalid load key, 'E'.`
- **Causa**: El archivo `stories260K.bin` tiene un tama帽o de 15 bytes y contiene el texto "Entry not found". Probablemente una descarga fallida de HuggingFace. Adem谩s, es un archivo `.bin` (C) intentando ser le铆do como `.pt` (PyTorch).
- **Soluci贸n**: Notificar al usuario y proporcionar links correctos para modelos `.pt`.

##  Registro: v1.2 - Inferencia Exitosa (Python)
- **Cambio**: Se descarg贸 el modelo `stories15M.pt` (58MB).
- **Resultado**: El script `sample.py` gener贸 texto correctamente: *"Once upon a time, there was a little girl named Lily..."*.
- **Estado**: La versi贸n de Python est谩 operativa.

##  Registro: v1.3 - Resoluci贸n de stories260K.pt
- **Fallo**: `AssertionError: data\tok512.model` e `IndexError`.
- **Causa**: 
    1. El modelo `stories260K` tiene un vocabulario de solo 512 tokens.
    2. Intentar usar el tokenizer de Llama2 (32,000 tokens) causa un error de 铆ndice en la capa de embeddings.
- **Soluci贸n**: Se descarg贸 el archivo `tok512.model` en la carpeta `data/`.
- **Resultado**: Inferencia exitosa con el modelo mini para pruebas.

##  Registro: v1.4 - Dataset de Ejemplo y Fine-tuning
- **Cambio**: Se cre贸 un dataset de ejemplo (`data/TinyStories_all_data/custom_data.json`) con 10 instrucciones.
- **Documentaci贸n**: Se a帽adieron los pasos para "Fine-tuning" al `project_status.md`, incluyendo pretokenizaci贸n y entrenamiento con `--init_from="resume"`.
- **Raz贸n**: El usuario desea saber c贸mo preparar sus propios datos para entrenar el modelo antes de llevarlo al ESP32.

##  Registro: v1.5 - Modalidades de Entrenamiento y C谩lculo de Tama帽o
- **A帽adido**: Explicaci贸n en `project_status.md` sobre la diferencia entre refinar (resume) y entrenar desde cero (scratch).
- **A帽adido**: Gu铆a de c谩lculo de par谩metros y peso del modelo para ESP32.
- **Raz贸n**: El usuario necesita saber c贸mo configurar el tama帽o exacto del modelo y calcular si cabr谩 en el hardware limitado del ESP32.

##  Registro: v1.6 - Correcci贸n de train.py para CPU
- **Fallo**: `RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False.`
- **Causa**: 
    1. `train.py` ten铆a `device = "cuda"` hardcodeado, lo que forzaba a `torch.load` a buscar una GPU incluso al usar `map_location`.
    2. Al igual que en `sample.py`, faltaba `weights_only=False` para PyTorch 2.6+.
- **Soluci贸n**: 
    1. Se hizo din谩mica la selecci贸n de `device` (CPU/CUDA) y `dtype`.
    2. Se a帽adi贸 `weights_only=False` a `torch.load` en `train.py`.
    3. Se desactiv贸 `compile` por defecto para evitar errores en Windows.

##  Registro: v1.7 - Gu铆a Maestra de Fine-tuning
- **Documentado**: Se cre贸 la "Gu铆a Maestra de Fine-tuning" en `project_status.md`.
- **Hallazgo**: El cargador de datos requiere al menos 2 shards binarios para el shuffle; se instruy贸 duplicar datos si el dataset es peque帽o.
- **Hallazgo**: `configurator.py` rompe la ejecuci贸n con espacios en los par谩metros de terminal; se recomend贸 el uso de guiones bajos (`_`).
- **Ajuste**: Se crearon las carpetas necesarias (`data/tok512/`) para que el entrenamiento encuentre los datos correctamente.

##  Registro: v1.8 - Consolidaci贸n de Gu铆a Multimodelo
- **Documentado**: Se reorganiz贸 `project_status.md` para distinguir claramente entre el modelo de ESP32 (260K) y el grande (15M).
- **A帽adido**: Instrucciones espec铆ficas de pretokenizaci贸n para ambos casos (vocab 512 vs 32000).
- **A帽adido**: Comandos de entrenamiento "desde cero" con par谩metros exactos para recrear el tama帽o 260K.
- **A帽adido**: Tabla Comparativa de par谩metros y requisitos de hardware.

##  Registro: v1.9 - Explicaci贸n de Train/Validation Split
- **A帽adido**: Explicaci贸n t茅cnica en `project_status.md` sobre c贸mo el script usa `sorted()` para asignar el Shard 0 a validaci贸n.
- **Recomendaci贸n**: Uso de prefijos num茅ricos (`00_val`, `01_train`) para controlar el flujo de datos sin depender del sistema de archivos.
- **Raz贸n**: El usuario ten铆a dudas sobre c贸mo el sistema diferenciaba entre entrenamiento y test, y si interven铆a el orden de creaci贸n del archivo.

##  Registro: v1.10 - Implementaci贸n de Shards y Prueba de Fine-tuning
- **Cambio**: Se dividi贸 el dataset en `00_val.json` (test) y `01_train.json` (entrenamiento).
- **xito**: El script `train.py` ejecut贸 correctamente un ciclo de entrenamiento (10 iters) usando `stories260K.pt` como base.
- **Validaci贸n**: Se confirm贸 que el modelo separa los Shards correctamente (el log mostr贸 p茅rdidas diferentes para train y val).

# Backup
*(Aqu铆 se guardar谩n ideas descartadas o versiones anteriores en el futuro)*
